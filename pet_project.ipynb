{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb1b9b6-ef54-4d31-b1bb-b82fa7563ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio.PDB as PDB\n",
    "import pathlib\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import biographs as bg\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Dataset, download_url, Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import freesasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3f89b6-a4be-461f-b2f9-1a3d5eda3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_res_table = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "# Dictionary for getting Residue symbols\n",
    "ressymbl = {'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU':'E', 'PHE': 'F', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN':'Q', 'ARG':'R', 'SER': 'S','THR': 'T', 'VAL': 'V', 'TRP':'W', 'TYR': 'Y'}\n",
    "sites_dict = {'INORGANIC_BINDING': 0,\n",
    " 'METAL_BINDING': 1,\n",
    " 'COMPOUND_BINDING': 2,\n",
    " 'UNCLASSIFIED': 3,\n",
    " 'MULTIPLE_LIGAND_BINDING': 4,\n",
    " 'COMPOUND_DRUG_BINDING': 5,\n",
    " 'ACTIVE_COMPLEXED': 6,\n",
    " 'ACTIVE': 7,\n",
    " 'PROTEIN_PROTEIN_INTERACTION': 8,\n",
    " 'PROTEIN_DNA_INTERACTION': 9,\n",
    " 'PROTEIN_RNA_INTERACTION': 10}\n",
    "aa_properties = {  # полярность(1)/неполярность(0); вандервальсов радиус; pI; Гидрофобность; Заряд; pKa(R), Normalized frequency of turn (Crawford et al., 1973), Normalized frequency of alpha-helix (Burgess et al., 1974)\n",
    "    'G': [0, 48, 6.06, -0.4, 0, 0, 1.38, 0.12],     # Глицин \n",
    "    'A': [0, 67, 6.01, 1.8, 0, 0, 0.6, 0.486],      # Аланин\n",
    "    'V': [0, 105, 6.00, 4.2, 0, 0, 0.48, 0.379],     # Валин\n",
    "    'I': [0, 124, 6.05, 4.5, 0, 0, 0.67, 0.37],     # Изолейцин\n",
    "    'L': [0, 124, 6.01, 3.8, 0, 0, 0.7, 0.42],     # Лейцин\n",
    "    'P': [0, 90, 6.30, -1.6, 0, 0, 1.47, 0.208],     # Пролин\n",
    "    'S': [1, 73, 5.68, -0.8, 0, 0, 1.26, 0.2],     # Серин\n",
    "    'T': [1, 93, 5.60, -0.7, 0, 0, 1.05, 0.272],     # Треонин\n",
    "    'C': [1, 86, 5.05, 2.5, 0, 8.33, 1.29, 0.2],   # Цистеин\n",
    "    'M': [0, 124, 5.74, 1.9, 0, 0, 0.67, 0.417],     # Метионин\n",
    "    'D': [1, 91, 2.85, -3.5, -1, 3.65, 1.24, 0.288], # Аспарагиновая кислота\n",
    "    'N': [1, 96, 5.41, -3.5, 0, 0, 1.42, 0.193],     # Аспарагин\n",
    "    'E': [1, 109, 3.15, -3.5, -1, 4.25, 0.64, 0.538],# Глутаминовая кислота\n",
    "    'Q': [1, 114, 5.65, -3.5, 0, 0, 0.92, 0.418],    # Глутамин\n",
    "    'K': [1, 135, 9.60, -3.9, 1, 10.28, 1.1, 0.402],# Лизин\n",
    "    'R': [1, 148, 10.76, -4.5, 1, 12.48, 0.79, 0.262],# Аргинин\n",
    "    'H': [1, 118, 7.60, -3.2, 1, 6.0, 0.95, 0.4], # Гистидин\n",
    "    'F': [0, 135, 5.49, 2.8, 0, 0, 1.05, 0.288],     # Фенилаланин\n",
    "    'Y': [1, 141, 5.64, -1.3, 0, 10.1, 1.35, 0.161], # Тирозин\n",
    "    'W': [0, 163, 5.89, -0.9, 0, 0, 1.23, 0.462]     # Триптофан\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eaa731b-4b36-4420-a671-e7ed8372974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_and_ca_coordinates(structure):\n",
    "    sequence =\"\"\n",
    "    sequence_dict = {}\n",
    "    count = 0\n",
    "    ca_coords = []\n",
    "    try:\n",
    "        result, _ = freesasa.calcBioPDB(structure)\n",
    "        residue_areas = result.residueAreas()\n",
    "    except:\n",
    "        filtered_structure = filter_unknown_residues(structure)\n",
    "        result, _ = freesasa.calcBioPDB(filtered_structure)\n",
    "        residue_areas = result.residueAreas()\n",
    "    sasa_dict = {}\n",
    "# for model in structure:\n",
    "    for chain in structure[0]:\n",
    "        for residue in chain:\n",
    "            if residue.id[0].startswith('H_'):\n",
    "                continue\n",
    "            if residue.get_resname() in ressymbl.keys():\n",
    "                sequence = sequence + ressymbl[residue.get_resname()]\n",
    "                print(residue.get_id())\n",
    "                if residue.get_id()[2] != ' ':\n",
    "                    sequence_dict[(str(residue.get_id()[1]) + residue.get_id()[2],chain.id)] = (ressymbl[residue.get_resname()],count)\n",
    "                    sasa_dict[(str(residue.get_id()[1]) + residue.get_id()[2],chain.id)] = residue_areas[chain.id][str(residue.get_id()[1]) + residue.get_id()[2]].total\n",
    "                else:\n",
    "                    sequence_dict[(str(residue.get_id()[1]),chain.id)] = (ressymbl[residue.get_resname()],count)\n",
    "                    sasa_dict[(str(residue.get_id()[1]),chain.id)] = residue_areas[chain.id][str(residue.get_id()[1])].total\n",
    "            \n",
    "                count += 1\n",
    "                try:                               #Если нет альфа-атома, то добавим координаты бета-атома остова аминокислоты\n",
    "                    ca_atom = residue[\"CA\"]   \n",
    "                    ca_coords.append(ca_atom.coord)\n",
    "                except:\n",
    "                    try:\n",
    "                        c_atom = residue['C']\n",
    "                        c_coords.append(cb_atom.coord)\n",
    "                    except:\n",
    "                        n_atom = residue['N']\n",
    "                        ca_coords.append(n_atom.coord)\n",
    "              \n",
    "    return sequence, sequence_dict, np.array(ca_coords), sasa_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de99a789-89b6-4611-801f-2e3806c42db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phis_chem_properties(seq):\n",
    "    aa_prop_is_seq = np.zeros([len(seq), len(aa_properties['A'])])\n",
    "    for i, residue in enumerate(seq):\n",
    "        if residue in aa_properties.keys():\n",
    "            aa_prop_is_seq[i,:] = aa_properties[residue]\n",
    "    return aa_prop_is_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3216aba2-de3b-47ef-ae93-f868feae4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_symbftrs(sequence):\n",
    "        one_hot_symb = np.zeros((len(sequence),len(pro_res_table)))\n",
    "        row= 0\n",
    "        for res in sequence:\n",
    "          col = pro_res_table.index(res)\n",
    "          one_hot_symb[row][col]=1\n",
    "          row +=1\n",
    "        return one_hot_symb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e91b0b7-9275-493d-bd31-07658137798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_labels(seq_dict, file):\n",
    "        y = torch.zeros((len(seq_dict), len(sites_dict)), dtype=torch.long)\n",
    "        for index, row in train_pdb_df[train_pdb_df['PDBID']== os.path.splitext(os.path.basename(file))[0]][['SITE_TYPE','AminoAcidsWithChains','POS_CHAINS']].iterrows():\n",
    "            pos_chain = eval(row['POS_CHAINS'])[0]\n",
    "            if pos_chain == \"YES\":\n",
    "                for amino_acid, position, chain in eval(row['AminoAcidsWithChains']):\n",
    "                    key = (position, chain)\n",
    "                    # print(key)\n",
    "                    if key in seq_dict and seq_dict[key][0] == amino_acid:\n",
    "                        # print(key)\n",
    "                        s_type = sites_dict[row['SITE_TYPE']]\n",
    "                        # print(s_type)\n",
    "                        y[seq_dict[key][1], s_type] = 1\n",
    "            else:\n",
    "                for amino_acid, position in eval(row['AminoAcidsWithChains']):\n",
    "                    new_seq_dict = {key[0]:val for  key,val in zip(seq_dict.keys(),seq_dict.values())}\n",
    "                    key = str(position)\n",
    "                    print(amino_acid, type(amino_acid))\n",
    "                    print(key, type(key))\n",
    "                    print(new_seq_dict[key])\n",
    "                    if key in new_seq_dict.keys() and new_seq_dict[key][0] == amino_acid:\n",
    "                        print(key, new_seq_dict[key], amino_acid)\n",
    "                        s_type = sites_dict[row['SITE_TYPE']]\n",
    "                        print(s_type)\n",
    "                        y[new_seq_dict[key][1], s_type] = 1\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5e7e71f5-9425-4bc8-b2e0-79c496871bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unknown_residues(structure):\n",
    "    \"\"\"Создает копию структуры без UNK, нуклеотидов и гетероатомов\"\"\"\n",
    "    filtered_structure = PDB.Structure.Structure(\"filtered\")\n",
    "    \n",
    "    for model in structure:\n",
    "        new_model = PDB.Model.Model(model.id)\n",
    "        for chain in model:\n",
    "            new_chain = PDB.Chain.Chain(chain.id)\n",
    "            for residue in chain:\n",
    "                resname = residue.get_resname().strip()\n",
    "                # Пропуск нуклеотидов, UNK и гетероатомов\n",
    "                if resname in ['A', 'C', 'G', 'T', 'U', 'N'] or resname == 'UNK' or residue.id[0] != ' ':\n",
    "                    continue\n",
    "                new_chain.add(residue)\n",
    "            new_model.add(new_chain)\n",
    "        filtered_structure.add(new_model)\n",
    "    \n",
    "    return filtered_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a54ebe-39d7-40ff-815f-14bf68290f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdb_df = pd.read_csv(\"train_df_full3_with_comb_classes_wo_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7883cef6-8857-43e9-91d3-49bc9eb0e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pdb_df = pd.read_csv('test_df_full3_with_comb_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025955c4-5082-4a3c-a644-43c928e7533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pdb_df = pd.read_csv('val_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae815b-c9b4-4336-95ce-73ec8717ff3f",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "710548a4-f562-4049-a7fa-bdc0befc8c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Graph_Dataset(Dataset):\n",
    "    def __init__(self, root, specific_files=None, transform=None, pre_transform=None):\n",
    "        self.root = root\n",
    "        self.specific_files = specific_files\n",
    "        self.custom_processed_dir = os.path.join(self.root, 'processed_type_4')\n",
    "        os.makedirs(self.custom_processed_dir, exist_ok=True)\n",
    "        super(Train_Graph_Dataset, self).__init__(root, transform=None, pre_transform=None)\n",
    "        self.data = self.processed_paths\n",
    "        self.data_prot = []\n",
    "          # Список конкретных файлов для обработки\n",
    "        \n",
    "        self._initialize_data()\n",
    "\n",
    "    def _initialize_data(self):\n",
    "        processed_files = [os.path.join(self.custom_processed_dir, os.path.splitext(os.path.basename(file))[0] + '.pt')\n",
    "                          for file in self.raw_paths]  # Используем self.raw_paths\n",
    "\n",
    "        if all(os.path.exists(f) for f in processed_files):\n",
    "            print(\"Все обработанные файлы уже существуют. Загружаем их.\")\n",
    "            self.data_prot = []\n",
    "            for file_path in processed_files:\n",
    "                try:\n",
    "                    data = torch.load(file_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "                    self.data_prot.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при загрузке {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "        else:\n",
    "            print(\"Некоторые обработанные файлы отсутствуют. Запускаем обработку.\")\n",
    "            self.process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        all_files = [f.path for f in os.scandir(self.root + \"/raw_files\") if f.is_file()]\n",
    "        if self.specific_files:\n",
    "            # Фильтруем файлы, оставляя только те, что есть в specific_files\n",
    "            return [f for f in all_files if f in self.specific_files]\n",
    "        else:\n",
    "            return all_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [os.path.splitext(os.path.basename(file))[0]+'.pt' for file in self.raw_paths]\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        if all(os.path.exists(os.path.join(self.custom_processed_dir, f)) for f in self.processed_file_names):\n",
    "            print(\"Все обработанные файлы уже существуют. Пропускаем повторную обработку.\")\n",
    "            return\n",
    "        self.data_prot = []  # Reset/clear data_prot\n",
    "        \n",
    "        data_list = []\n",
    "        files_with_errors = []\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        count3 = 0\n",
    "        count4 = 0\n",
    "        print(f\"Найдено файлов для обработки: {len(self.raw_paths)}\")\n",
    "        \n",
    "        for file in tqdm(self.raw_paths):\n",
    "            \n",
    "            if pathlib.Path(file).suffix == \".pdb\":\n",
    "                print(f\"\\nОбработка файла: {file}\")\n",
    "                try:\n",
    "                    struct = self._get_structure(file)\n",
    "                    print(f\"Структура загружена успешно\")\n",
    "                    \n",
    "                    seq, seq_dict, Ca_coords, sasa_dict = self._get_sequence_and_ca_coordinates(struct)\n",
    "                    print(f\"Последовательность получена\")\n",
    "                    \n",
    "                    node_feats = self._get_node_ftrs(seq, struct, file, sasa_dict)\n",
    "                    print(f\"Размер признаков узлов: {node_feats.shape}\")\n",
    "                    \n",
    "                    mat = self._get_adjacency(Ca_coords)\n",
    "                    print(\"Матрица смежности получена\")\n",
    "                    edge_index = self._get_edgeindex(mat)\n",
    "                    print(\"Список список ребер получен\")\n",
    "                    print(f\"Размер матрицы смежности: {mat.shape}\")\n",
    "                    print(f\"Размер матрицы узлов: {node_feats.shape}\")\n",
    "                    if mat.shape[0] == node_feats.shape[0]:\n",
    "                        count3 += 1\n",
    "                    else:\n",
    "                        count4 += 1\n",
    "\n",
    "                    labels = self._get_node_labels(seq_dict, file)\n",
    "                    # print(labels.nonzero())\n",
    "                    print('Метки получены')\n",
    "                    \n",
    "                    data = Data(x=node_feats, edge_index=edge_index, y = labels)\n",
    "                    data_list.append(data)\n",
    "                    count1 += 1\n",
    "                    \n",
    "                    torch.save(data, self.custom_processed_dir + \"/\" + \n",
    "                             os.path.splitext(os.path.basename(file))[0] + '.pt')\n",
    "                    print(f\"Данные сохранены успешно {file}\")\n",
    "                    print(\"---------------------\")\n",
    "                    # print(count1)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    files_with_errors.append(file)\n",
    "                    print(f\"Ошибка при обработке {file}: {str(e)}\")\n",
    "                    count2 += 1\n",
    "                    # print(count2)\n",
    "                    continue\n",
    "                \n",
    "        self.data_prot = data_list\n",
    "        print(self.custom_processed_dir)\n",
    "        print(f\"\\nИтоговая статистика:\")\n",
    "        print(f\"Успешно обработано файлов: {count1}\")\n",
    "        print(f\"Ошибок при обработке: {count2}\")\n",
    "        print(f\"Количество файлов с которых матрицы смежности и узлов равны {count3}\")\n",
    "        print(f\"Количество файлов с которых матрицы смежности и узлов не равны {count4}\")\n",
    "        print(f\"Размер data_prot: {len(self.data_prot)}\")\n",
    "        print(files_with_errors)\n",
    "    def __len__(self):\n",
    "        return len(self.data_prot)\n",
    "    \n",
    "    # file stands for file path\n",
    "    def __getitem__(self, idx):\n",
    "        # print(idx)\n",
    "        # print(len(self.data_prot))\n",
    "     \n",
    "        return self.data_prot[idx] \n",
    "     \n",
    "    def _get_adjacency(self, ca_coords):\n",
    "        ca_distance = np.linalg.norm(ca_coords[:, None, :] - ca_coords[None, :, :], axis=-1) + np.eye(ca_coords.shape[0])*6\n",
    "        adjacency_matrix = ca_distance < 6\n",
    "        # network = nx.from_numpy_array(mask)\n",
    "        \n",
    "        #прошлый кусок кода, который пока не убираю\n",
    "        # network = molecule.network()  \n",
    "        # mat = nx.adjacency_matrix(network)\n",
    "        # m = mat.todense()\n",
    "        return adjacency_matrix\n",
    "   \n",
    "    def _get_edgeindex(self, adjacency_matrix):\n",
    "        nx_graph = nx.from_numpy_array(adjacency_matrix)\n",
    "        edge_index = np.array(nx_graph.edges()).T\n",
    "        return torch.tensor(edge_index, dtype=torch.long)\n",
    "        \n",
    "        #Прошлый кусок кода\n",
    "        \n",
    "        # a, b = np.nonzero(adjacency_mat > 0)\n",
    "        # edge_index = np.stack((a, b), axis=0)  # Создаем массив (2, num_edges)\n",
    "        # return torch.tensor(edge_index, dtype=torch.long)\n",
    "        # edge_ind = []\n",
    "        \n",
    "    def _get_structure(self, file):\n",
    "        parser = PDB.PDBParser()\n",
    "        structure = parser.get_structure(id, file)\n",
    "        return structure\n",
    "\n",
    "    def _filter_unknown_residues(self, structure):\n",
    "        \"\"\"Создает копию структуры без UNK, нуклеотидов и гетероатомов\"\"\"\n",
    "        filtered_structure = PDB.Structure.Structure(\"filtered\")\n",
    "        \n",
    "        for model in structure:\n",
    "            new_model = PDB.Model.Model(model.id)\n",
    "            for chain in model:\n",
    "                new_chain = PDB.Chain.Chain(chain.id)\n",
    "                for residue in chain:\n",
    "                    resname = residue.get_resname().strip()\n",
    "                    # Пропуск нуклеотидов, UNK и гетероатомов\n",
    "                    if resname in ['A', 'C', 'G', 'T', 'U', 'N'] or resname == 'UNK' or residue.id[0] != ' ':\n",
    "                        continue\n",
    "                    new_chain.add(residue)\n",
    "                new_model.add(new_chain)\n",
    "            filtered_structure.add(new_model)\n",
    "        \n",
    "        return filtered_structure\n",
    "\n",
    "\n",
    "    # Function to get sequence from pdb structure \n",
    "    # Uses structure made using biopython\n",
    "    # Those residues for which symbols are U / X are converted into A\n",
    "    \n",
    "    def _get_sequence_and_ca_coordinates(self, structure):\n",
    "        sequence =\"\"\n",
    "        sequence_dict = {}\n",
    "        count = 0\n",
    "        ca_coords = []\n",
    "        try:\n",
    "            result, _ = freesasa.calcBioPDB(structure)\n",
    "            residue_areas = result.residueAreas()\n",
    "        except:\n",
    "            filtered_structure = self.filter_unknown_residues(structure)\n",
    "            result, _ = freesasa.calcBioPDB(filtered_structure)\n",
    "            residue_areas = result.residueAreas()\n",
    "        sasa_dict = {}\n",
    "    # for model in structure:\n",
    "        for chain in structure[0]:\n",
    "            for residue in chain:\n",
    "                if residue.get_resname() in ressymbl.keys():\n",
    "                    if residue.id[0].startswith('H_'):\n",
    "                        continue\n",
    "                    sequence = sequence + ressymbl[residue.get_resname()]\n",
    "                    if residue.get_id()[2] != ' ':\n",
    "                        sequence_dict[(str(residue.get_id()[1]) + residue.get_id()[2],chain.id)] = (ressymbl[residue.get_resname()],count)\n",
    "                        sasa_dict[(str(residue.get_id()[1]) + residue.get_id()[2],chain.id)] = residue_areas[chain.id][str(residue.get_id()[1]) + residue.get_id()[2]].total\n",
    "                    else:\n",
    "                        sequence_dict[(str(residue.get_id()[1]),chain.id)] = (ressymbl[residue.get_resname()],count)\n",
    "                        sasa_dict[(str(residue.get_id()[1]),chain.id)] = residue_areas[chain.id][str(residue.get_id()[1])].total\n",
    "                    count += 1\n",
    "                    try:                               #Если нет альфа-атома, то добавим координаты бета-атома остова аминокислоты\n",
    "                        ca_atom = residue[\"CA\"]   \n",
    "                        ca_coords.append(ca_atom.coord)\n",
    "                    except:\n",
    "                        try:\n",
    "                            c_atom = residue['C']\n",
    "                            c_coords.append(cb_atom.coord)\n",
    "                        except:\n",
    "                            n_atom = residue['N']\n",
    "                            ca_coords.append(n_atom.coord)\n",
    "                  \n",
    "        return sequence, sequence_dict, np.array(ca_coords), sasa_dict\n",
    "    \n",
    "\n",
    "    # One hot encoding for symbols\n",
    "    def _get_one_hot_symbftrs(self, sequence):\n",
    "        one_hot_symb = np.zeros((len(sequence),len(pro_res_table)))\n",
    "        row = 0\n",
    "        for res in sequence:\n",
    "          col = pro_res_table.index(res)\n",
    "          one_hot_symb[row][col]=1\n",
    "          row +=1\n",
    "        return one_hot_symb\n",
    "    \n",
    "    def _get_phis_chem_properties(self, sequence):\n",
    "        aa_prop_is_seq = np.zeros((len(sequence), len(aa_properties['A'])))\n",
    "        for i, residue in enumerate(sequence):\n",
    "            if residue in aa_properties.keys():\n",
    "                aa_prop_is_seq[i,:] = aa_properties[residue]\n",
    "        return aa_prop_is_seq\n",
    "\n",
    "    def _get_sasa(self, sasa_dict, sequence):\n",
    "        if len(sasa_dict) == len(sequence):\n",
    "            sasa = np.zeros(len(sasa_dict))\n",
    "            for i, key in enumerate(sasa_dict.keys()):\n",
    "                sasa[i] = sasa_dict[key]\n",
    "            return sasa\n",
    "\n",
    "    def _handle_sequence_mismatch(self, sequence, ang_seq, angles):\n",
    "        \"\"\"Синхронизирует углы с основной последовательностью\"\"\"\n",
    "        full_angles = np.zeros((len(sequence), angles.shape[1]))\n",
    "        res_idx = 0\n",
    "        \n",
    "        for i, residue in enumerate(sequence):\n",
    "            if res_idx < len(angles) and residue == ang_seq[res_idx]:\n",
    "                full_angles[i,:] = angles[res_idx]\n",
    "                res_idx += 1\n",
    "            else:\n",
    "                full_angles[i,:] = np.array([1,0,1,0])  # Заполнитель\n",
    "        \n",
    "        return full_angles\n",
    "\n",
    "    def _get_node_ftrs(self, sequence, structure, file, sasa_dict):\n",
    "        try:\n",
    "            one_hot_symb = self._get_one_hot_symbftrs(sequence)\n",
    "            # print('one-hot done')\n",
    "            phis_chem_properties = self._get_phis_chem_properties(sequence)\n",
    "            # print('phis done')\n",
    "            sasa = self._get_sasa(sasa_dict, sequence)\n",
    "            # print('sasa done')\n",
    "            angles, ang_seq = self._get_angles(structure[0])\n",
    "            \n",
    "            # Синхронизация длин последовательностей\n",
    "            if len(sequence) != len(ang_seq):\n",
    "                print('angles rewrite')\n",
    "                angles = self._handle_sequence_mismatch(sequence, ang_seq, angles)\n",
    "            return torch.tensor(np.hstack((one_hot_symb, angles, phis_chem_properties, sasa.reshape(-1,1))), dtype = torch.float)\n",
    "                    # else:\n",
    "                    #     one_hot_symb = self._get_one_hot_symbftrs(ang_seq)\n",
    "                    #     phis_chem_properties = self._get_phis_chem_properties(sequence)\n",
    "                    #     return torch.tensor(np.hstack((one_hot_symb, angles, phis_chem_properties)), dtype = torch.float)\n",
    "        except Exception as e:\n",
    "    \n",
    "                    print(f\"Ошибка при обработке {file}: {str(e)}\") \n",
    "\n",
    "    def _get_angles(self, structure):\n",
    "        angles_trig = []\n",
    "        seq = \"\"\n",
    "\n",
    "        # Проходим по моделям и цепям\n",
    "    # for model in structure:\n",
    "    #     for chain in model:\n",
    "        for chain in structure:    \n",
    "            polypeptides = PDB.CaPPBuilder().build_peptides(chain)\n",
    "            for poly in polypeptides:\n",
    "                seq += poly.get_sequence()\n",
    "                phi_psi = poly.get_phi_psi_list()\n",
    "                phi_psi[0] = (0, phi_psi[0][1])\n",
    "                phi_psi[-1] = (phi_psi[-1][0], 0)\n",
    "                phi_psi = np.array(phi_psi)\n",
    "                try:\n",
    "                    sin_phi = np.sin(phi_psi[:,0])\n",
    "                except TypeError:\n",
    "                    phi_psi = np.array([[0 if value is None else value for value in item] for item in phi_psi])\n",
    "                    sin_phi = np.sin(phi_psi[:,0])\n",
    "                cos_phi = np.cos(phi_psi[:,0])\n",
    "                sin_psi = np.sin(phi_psi[:,1])\n",
    "                cos_psi = np.cos(phi_psi[:,1])\n",
    "                sin_cos_poly = np.column_stack((sin_phi,cos_phi,sin_psi, cos_psi))\n",
    "                angles_trig.append(sin_cos_poly)\n",
    "        return np.vstack(angles_trig), seq\n",
    "\n",
    "    def _get_node_labels(self, seq_dict, file):\n",
    "        y = torch.zeros((len(seq_dict), len(sites_dict.keys())), dtype=torch.long)\n",
    "        for index, row in train_pdb_df[train_pdb_df['PDBID']== os.path.splitext(os.path.basename(file))[0]][['SITE_TYPE','AminoAcidsWithChains','POS_CHAINS']].iterrows():\n",
    "            pos_chain = eval(row['POS_CHAINS'])[0]\n",
    "            if pos_chain == \"YES\":\n",
    "                for amino_acid, position, chain in eval(row['AminoAcidsWithChains']):\n",
    "                    key = (position, chain)\n",
    "                    # print(key)\n",
    "                    if key in seq_dict and seq_dict[key][0] == amino_acid:\n",
    "                        # print(key)\n",
    "                        s_type = sites_dict[row['SITE_TYPE']]\n",
    "                        # print(s_type)\n",
    "                        y[seq_dict[key][1], s_type] = 1\n",
    "            else:\n",
    "                for amino_acid, position in eval(row['AminoAcidsWithChains']):\n",
    "                    new_seq_dict = {key[0]:val for  key,val in zip(seq_dict.keys(),seq_dict.values())}\n",
    "                    key = str(position)\n",
    "                    if key in new_seq_dict.keys() and new_seq_dict[key][0] == amino_acid:\n",
    "                        print(key, new_seq_dict[key], amino_acid)\n",
    "                        s_type = sites_dict[row['SITE_TYPE']]\n",
    "                        y[new_seq_dict[key][1], s_type] = 1\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b156925-de02-4079-9a0e-6f004f51c6c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все обработанные файлы уже существуют. Загружаем их.\n"
     ]
    }
   ],
   "source": [
    "train_dataset =Train_Graph_Dataset(root=r\"D:/Proteins/train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b18738-0fe6-4baf-aecc-484951ab2773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Val_Graph_Dataset(Dataset):\n",
    "    def __init__(self, root, specific_files=None, transform=None, pre_transform=None):\n",
    "        self.root = root\n",
    "        self.specific_files = specific_files\n",
    "        self.custom_processed_dir = os.path.join(self.root, 'processed_type_4')\n",
    "          # Список конкретных файлов для обработки\n",
    "        os.makedirs(self.custom_processed_dir, exist_ok=True)\n",
    "        super(Val_Graph_Dataset, self).__init__(root, transform=None, pre_transform=None)\n",
    "        self.data = self.processed_paths\n",
    "        self.data_prot = []\n",
    "        \n",
    "        self._initialize_data()\n",
    "\n",
    "    def _initialize_data(self):\n",
    "        processed_files = [os.path.join(self.custom_processed_dir, os.path.splitext(os.path.basename(file))[0] + '.pt')\n",
    "                          for file in self.raw_paths]  # Используем self.raw_paths\n",
    "\n",
    "        if all(os.path.exists(f) for f in processed_files):\n",
    "            print(\"Все обработанные файлы уже существуют. Загружаем их.\")\n",
    "            self.data_prot = []\n",
    "            for file_path in processed_files:\n",
    "                try:\n",
    "                    data = torch.load(file_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "                    self.data_prot.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при загрузке {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "        else:\n",
    "            print(\"Некоторые обработанные файлы отсутствуют. Запускаем обработку.\")\n",
    "            self.process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        all_files = [f.path for f in os.scandir(self.root + \"/raw_files\") if f.is_file()]\n",
    "        if self.specific_files:\n",
    "            # Фильтруем файлы, оставляя только те, что есть в specific_files\n",
    "            return [f for f in all_files if f in self.specific_files]\n",
    "        else:\n",
    "            return all_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [os.path.splitext(os.path.basename(file))[0]+'.pt' for file in self.raw_paths]\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        if all(os.path.exists(os.path.join(self.custom_processed_dir, f)) for f in self.processed_file_names):\n",
    "            print(\"Все обработанные файлы уже существуют. Пропускаем повторную обработку.\")\n",
    "            return\n",
    "        self.data_prot = []  # Reset/clear data_prot\n",
    "        \n",
    "        data_list = []\n",
    "        files_with_errors = []\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        count3 = 0\n",
    "        count4 = 0\n",
    "        print(f\"Найдено файлов для обработки: {len(self.raw_paths)}\")\n",
    "        \n",
    "        for file in tqdm(self.raw_paths):\n",
    "            \n",
    "            if pathlib.Path(file).suffix == \".pdb\":\n",
    "                print(f\"\\nОбработка файла: {file}\")\n",
    "                try:\n",
    "                    struct = self._get_structure(file)\n",
    "                    print(f\"Структура загружена успешно\")\n",
    "                    \n",
    "                    seq, seq_dict, Ca_coords, sasa_dict = self._get_sequence_and_ca_coordinates(struct)\n",
    "                    print(f\"Последовательность получена\")\n",
    "                    \n",
    "                    node_feats = self._get_node_ftrs(seq, struct, file, sasa_dict)\n",
    "                    print(f\"Размер признаков узлов: {node_feats.shape}\")\n",
    "                    \n",
    "                    mat = self._get_adjacency(Ca_coords)\n",
    "                    print(\"Матрица смежности получена\")\n",
    "                    edge_index = self._get_edgeindex(mat)\n",
    "                    print(\"Список список ребер получен\")\n",
    "                    print(f\"Размер матрицы смежности: {mat.shape}\")\n",
    "                    print(f\"Размер матрицы узлов: {node_feats.shape}\")\n",
    "                    if mat.shape[0] == node_feats.shape[0]:\n",
    "                        count3 += 1\n",
    "                    else:\n",
    "                        count4 += 1\n",
    "\n",
    "                    labels = self._get_node_labels(seq_dict, file)\n",
    "                    # print(labels.nonzero())\n",
    "                    print('Метки получены')\n",
    "                    \n",
    "                    data = Data(x=node_feats, edge_index=edge_index, y = labels)\n",
    "                    data_list.append(data)\n",
    "                    count1 += 1\n",
    "                    \n",
    "                    torch.save(data, self.custom_processed_dir + \"/\" + \n",
    "                             os.path.splitext(os.path.basename(file))[0] + '.pt')\n",
    "                    print(f\"Данные сохранены успешно {file}\")\n",
    "                    print(\"---------------------\")\n",
    "                    # print(count1)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    files_with_errors.append(file)\n",
    "                    print(f\"Ошибка при обработке {file}: {str(e)}\")\n",
    "                    count2 += 1\n",
    "                    # print(count2)\n",
    "                    continue\n",
    "                \n",
    "        self.data_prot = data_list\n",
    "        print(self.custom_processed_dir)\n",
    "        print(f\"\\nИтоговая статистика:\")\n",
    "        print(f\"Успешно обработано файлов: {count1}\")\n",
    "        print(f\"Ошибок при обработке: {count2}\")\n",
    "        print(f\"Количество файлов с которых матрицы смежности и узлов равны {count3}\")\n",
    "        print(f\"Количество файлов с которых матрицы смежности и узлов не равны {count4}\")\n",
    "        print(f\"Размер data_prot: {len(self.data_prot)}\")\n",
    "        print(files_with_errors)\n",
    "    def __len__(self):\n",
    "        return len(self.data_prot)\n",
    "    \n",
    "    # file stands for file path\n",
    "    def __getitem__(self, idx):\n",
    "        # print(idx)\n",
    "        # print(len(self.data_prot))\n",
    "     \n",
    "        return self.data_prot[idx] \n",
    "     \n",
    "    def _get_adjacency(self, ca_coords):\n",
    "        ca_distance = np.linalg.norm(ca_coords[:, None, :] - ca_coords[None, :, :], axis=-1) + np.eye(ca_coords.shape[0])*6\n",
    "        adjacency_matrix = ca_distance < 6\n",
    "        # network = nx.from_numpy_array(mask)\n",
    "        \n",
    "        #прошлый кусок кода, который пока не убираю\n",
    "        # network = molecule.network()  \n",
    "        # mat = nx.adjacency_matrix(network)\n",
    "        # m = mat.todense()\n",
    "        return adjacency_matrix\n",
    "   \n",
    "    def _get_edgeindex(self, adjacency_matrix):\n",
    "        nx_graph = nx.from_numpy_array(adjacency_matrix)\n",
    "        edge_index = np.array(nx_graph.edges()).T\n",
    "        return torch.tensor(edge_index, dtype=torch.long)\n",
    "        \n",
    "        #Прошлый кусок кода\n",
    "        \n",
    "        # a, b = np.nonzero(adjacency_mat > 0)\n",
    "        # edge_index = np.stack((a, b), axis=0)  # Создаем массив (2, num_edges)\n",
    "        # return torch.tensor(edge_index, dtype=torch.long)\n",
    "        # edge_ind = []\n",
    "        \n",
    "    def _get_structure(self, file):\n",
    "        parser = PDB.PDBParser()\n",
    "        structure = parser.get_structure(id, file)\n",
    "        return structure\n",
    "\n",
    "    def _filter_unknown_residues(self, structure):\n",
    "        \"\"\"Создает копию структуры без UNK, нуклеотидов и гетероатомов\"\"\"\n",
    "        filtered_structure = PDB.Structure.Structure(\"filtered\")\n",
    "        \n",
    "        for model in structure:\n",
    "            new_model = PDB.Model.Model(model.id)\n",
    "            for chain in model:\n",
    "                new_chain = PDB.Chain.Chain(chain.id)\n",
    "                for residue in chain:\n",
    "                    resname = residue.get_resname().strip()\n",
    "                    # Пропуск нуклеотидов, UNK и гетероатомов\n",
    "                    if resname in ['A', 'C', 'G', 'T', 'U', 'N'] or resname == 'UNK' or residue.id[0] != ' ':\n",
    "                        continue\n",
    "                    new_chain.add(residue)\n",
    "                new_model.add(new_chain)\n",
    "            filtered_structure.add(new_model)\n",
    "        \n",
    "        return filtered_structure\n",
    "\n",
    "\n",
    "    # Function to get sequence from pdb structure \n",
    "    # Uses structure made using biopython\n",
    "    # Those residues for which symbols are U / X are converted into A\n",
    "    \n",
    "    def _get_sequence_and_ca_coordinates(self, structure):\n",
    "        sequence =\"\"\n",
    "        sequence_dict = {}\n",
    "        count = 0\n",
    "        ca_coords = []\n",
    "        try:\n",
    "            result, _ = freesasa.calcBioPDB(structure)\n",
    "            residue_areas = result.residueAreas()\n",
    "        except:\n",
    "            filtered_structure = self.filter_unknown_residues(structure)\n",
    "            result, _ = freesasa.calcBioPDB(filtered_structure)\n",
    "            residue_areas = result.residueAreas()\n",
    "        sasa_dict = {}\n",
    "    # for model in structure:\n",
    "        for chain in structure[0]:\n",
    "            for residue in chain:\n",
    "                if residue.get_resname() in ressymbl.keys():\n",
    "                    if residue.id[0].startswith('H_'):\n",
    "                        continue\n",
    "                    sequence = sequence + ressymbl[residue.get_resname()]\n",
    "                    if residue.get_id()[2] != ' ':\n",
    "                        sequence_dict[(str(residue.get_id()[1]) + residue.get_id()[2],chain.id)] = (ressymbl[residue.get_resname()],count)\n",
    "                        sasa_dict[(str(residue.get_id()[1]) + residue.get_id()[2],chain.id)] = residue_areas[chain.id][str(residue.get_id()[1]) + residue.get_id()[2]].total\n",
    "                    else:\n",
    "                        sequence_dict[(str(residue.get_id()[1]),chain.id)] = (ressymbl[residue.get_resname()],count)\n",
    "                        sasa_dict[(str(residue.get_id()[1]),chain.id)] = residue_areas[chain.id][str(residue.get_id()[1])].total\n",
    "                    count += 1\n",
    "                    try:                               #Если нет альфа-атома, то добавим координаты бета-атома остова аминокислоты\n",
    "                        ca_atom = residue[\"CA\"]   \n",
    "                        ca_coords.append(ca_atom.coord)\n",
    "                    except:\n",
    "                        try:\n",
    "                            c_atom = residue['C']\n",
    "                            c_coords.append(cb_atom.coord)\n",
    "                        except:\n",
    "                            n_atom = residue['N']\n",
    "                            ca_coords.append(n_atom.coord)\n",
    "                  \n",
    "        return sequence, sequence_dict, np.array(ca_coords), sasa_dict\n",
    "    \n",
    "\n",
    "    # One hot encoding for symbols\n",
    "    def _get_one_hot_symbftrs(self, sequence):\n",
    "        one_hot_symb = np.zeros((len(sequence),len(pro_res_table)))\n",
    "        row = 0\n",
    "        for res in sequence:\n",
    "          col = pro_res_table.index(res)\n",
    "          one_hot_symb[row][col]=1\n",
    "          row +=1\n",
    "        return one_hot_symb\n",
    "    \n",
    "    def _get_phis_chem_properties(self, sequence):\n",
    "        aa_prop_is_seq = np.zeros((len(sequence), len(aa_properties['A'])))\n",
    "        for i, residue in enumerate(sequence):\n",
    "            if residue in aa_properties.keys():\n",
    "                aa_prop_is_seq[i,:] = aa_properties[residue]\n",
    "        return aa_prop_is_seq\n",
    "\n",
    "    def _get_sasa(self, sasa_dict, sequence):\n",
    "        if len(sasa_dict) == len(sequence):\n",
    "            sasa = np.zeros(len(sasa_dict))\n",
    "            for i, key in enumerate(sasa_dict.keys()):\n",
    "                sasa[i] = sasa_dict[key]\n",
    "            return sasa\n",
    "\n",
    "    def _handle_sequence_mismatch(self, sequence, ang_seq, angles):\n",
    "        \"\"\"Синхронизирует углы с основной последовательностью\"\"\"\n",
    "        full_angles = np.zeros((len(sequence), angles.shape[1]))\n",
    "        res_idx = 0\n",
    "        \n",
    "        for i, residue in enumerate(sequence):\n",
    "            if res_idx < len(angles) and residue == ang_seq[res_idx]:\n",
    "                full_angles[i,:] = angles[res_idx]\n",
    "                res_idx += 1\n",
    "            else:\n",
    "                full_angles[i,:] = np.array([1,0,1,0])  # Заполнитель\n",
    "        \n",
    "        return full_angles\n",
    "\n",
    "    def _get_node_ftrs(self, sequence, structure, file, sasa_dict):\n",
    "        try:\n",
    "            one_hot_symb = self._get_one_hot_symbftrs(sequence)\n",
    "            # print('one-hot done')\n",
    "            phis_chem_properties = self._get_phis_chem_properties(sequence)\n",
    "            # print('phis done')\n",
    "            sasa = self._get_sasa(sasa_dict, sequence)\n",
    "            # print('sasa done')\n",
    "            angles, ang_seq = self._get_angles(structure[0])\n",
    "            \n",
    "            # Синхронизация длин последовательностей\n",
    "            if len(sequence) != len(ang_seq):\n",
    "                print('angles rewrite')\n",
    "                angles = self._handle_sequence_mismatch(sequence, ang_seq, angles)\n",
    "            return torch.tensor(np.hstack((one_hot_symb, angles, phis_chem_properties, sasa.reshape(-1,1))), dtype = torch.float)\n",
    "                    # else:\n",
    "                    #     one_hot_symb = self._get_one_hot_symbftrs(ang_seq)\n",
    "                    #     phis_chem_properties = self._get_phis_chem_properties(sequence)\n",
    "                    #     return torch.tensor(np.hstack((one_hot_symb, angles, phis_chem_properties)), dtype = torch.float)\n",
    "        except Exception as e:\n",
    "    \n",
    "                    print(f\"Ошибка при обработке {file}: {str(e)}\") \n",
    "\n",
    "    def _get_angles(self, structure):\n",
    "        angles_trig = []\n",
    "        seq = \"\"\n",
    "\n",
    "        # Проходим по моделям и цепям\n",
    "    # for model in structure:\n",
    "    #     for chain in model:\n",
    "        for chain in structure:    \n",
    "            polypeptides = PDB.CaPPBuilder().build_peptides(chain)\n",
    "            for poly in polypeptides:\n",
    "                seq += poly.get_sequence()\n",
    "                phi_psi = poly.get_phi_psi_list()\n",
    "                phi_psi[0] = (0, phi_psi[0][1])\n",
    "                phi_psi[-1] = (phi_psi[-1][0], 0)\n",
    "                phi_psi = np.array(phi_psi)\n",
    "                try:\n",
    "                    sin_phi = np.sin(phi_psi[:,0])\n",
    "                except TypeError:\n",
    "                    phi_psi = np.array([[0 if value is None else value for value in item] for item in phi_psi])\n",
    "                    sin_phi = np.sin(phi_psi[:,0])\n",
    "                cos_phi = np.cos(phi_psi[:,0])\n",
    "                sin_psi = np.sin(phi_psi[:,1])\n",
    "                cos_psi = np.cos(phi_psi[:,1])\n",
    "                sin_cos_poly = np.column_stack((sin_phi,cos_phi,sin_psi, cos_psi))\n",
    "                angles_trig.append(sin_cos_poly)\n",
    "        return np.vstack(angles_trig), seq\n",
    "\n",
    "    def _get_node_labels(self, seq_dict, file):\n",
    "        y = torch.zeros((len(seq_dict), len(sites_dict.keys())), dtype=torch.long)\n",
    "        for index, row in val_pdb_df[val_pdb_df['PDBID']== os.path.splitext(os.path.basename(file))[0]][['SITE_TYPE','AminoAcidsWithChains','POS_CHAINS']].iterrows():\n",
    "            pos_chain = eval(row['POS_CHAINS'])[0]\n",
    "            if pos_chain == \"YES\":\n",
    "                for amino_acid, position, chain in eval(row['AminoAcidsWithChains']):\n",
    "                    key = (position, chain)\n",
    "                    # print(key)\n",
    "                    if key in seq_dict and seq_dict[key][0] == amino_acid:\n",
    "                        # print(key)\n",
    "                        s_type = sites_dict[row['SITE_TYPE']]\n",
    "                        # print(s_type)\n",
    "                        y[seq_dict[key][1], s_type] = 1\n",
    "            else:\n",
    "                for amino_acid, position in eval(row['AminoAcidsWithChains']):\n",
    "                    new_seq_dict = {key[0]:val for  key,val in zip(seq_dict.keys(),seq_dict.values())}\n",
    "                    key = str(position)\n",
    "                    if key in new_seq_dict.keys() and new_seq_dict[key][0] == amino_acid:\n",
    "                        print(key, new_seq_dict[key], amino_acid)\n",
    "                        s_type = sites_dict[row['SITE_TYPE']]\n",
    "                        y[new_seq_dict[key][1], s_type] = 1\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8239a2-459b-4ca6-b45a-11b83ff64a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_dataset = Val_Graph_Dataset(root=r\"D:/Proteins/val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba3d9c-38c4-43a9-839a-69228631bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Graph_Dataset(Dataset):\n",
    "    def __init__(self, root, specific_files=None, transform=None, pre_transform=None):\n",
    "        self.root = root\n",
    "        self.specific_files = specific_files\n",
    "        self.custom_processed_dir = os.path.join(self.root, 'processed_type_4')\n",
    "          # Список конкретных файлов для обработки\n",
    "        os.makedirs(self.custom_processed_dir, exist_ok=True)\n",
    "        super(Test_Graph_Dataset, self).__init__(root, transform=None, pre_transform=None)\n",
    "        self.data = self.processed_paths\n",
    "        self.data_prot = []\n",
    "        \n",
    "        self._initialize_data()\n",
    "\n",
    "    def _initialize_data(self):\n",
    "        processed_files = [os.path.join(self.custom_processed_dir, os.path.splitext(os.path.basename(file))[0] + '.pt')\n",
    "                          for file in self.raw_paths]  # Используем self.raw_paths\n",
    "\n",
    "        if all(os.path.exists(f) for f in processed_files):\n",
    "            print(\"Все обработанные файлы уже существуют. Загружаем их.\")\n",
    "            self.data_prot = []\n",
    "            for file_path in processed_files:\n",
    "                try:\n",
    "                    data = torch.load(file_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "                    self.data_prot.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при загрузке {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "        else:\n",
    "            print(\"Некоторые обработанные файлы отсутствуют. Запускаем обработку.\")\n",
    "            self.process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        all_files = [f.path for f in os.scandir(self.root + \"/raw_files\") if f.is_file()]\n",
    "        if self.specific_files:\n",
    "            # Фильтруем файлы, оставляя только те, что есть в specific_files\n",
    "            return [f for f in all_files if f in self.specific_files]\n",
    "        else:\n",
    "            return all_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [os.path.splitext(os.path.basename(file))[0]+'.pt' for file in self.raw_paths]\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        if all(os.path.exists(os.path.join(self.custom_processed_dir, f)) for f in self.processed_file_names):\n",
    "            print(\"Все обработанные файлы уже существуют. Пропускаем повторную обработку.\")\n",
    "            return\n",
    "        self.data_prot = []  # Reset/clear data_prot\n",
    "        \n",
    "        data_list = []\n",
    "        files_with_errors = []\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        count3 = 0\n",
    "        count4 = 0\n",
    "        print(f\"Найдено файлов для обработки: {len(self.raw_paths)}\")\n",
    "        \n",
    "        for file in tqdm(self.raw_paths):\n",
    "            \n",
    "            if pathlib.Path(file).suffix == \".pdb\":\n",
    "                print(f\"\\nОбработка файла: {file}\")\n",
    "                try:\n",
    "                    struct = self._get_structure(file)\n",
    "                    print(f\"Структура загружена успешно\")\n",
    "                    \n",
    "                    seq, seq_dict, Ca_coords, sasa_dict = self._get_sequence_and_ca_coordinates(struct)\n",
    "                    print(f\"Последовательность получена\")\n",
    "                    \n",
    "                    node_feats = self._get_node_ftrs(seq, struct, file, sasa_dict)\n",
    "                    print(f\"Размер признаков узлов: {node_feats.shape}\")\n",
    "                    \n",
    "                    mat = self._get_adjacency(Ca_coords)\n",
    "                    print(\"Матрица смежности получена\")\n",
    "                    edge_index = self._get_edgeindex(mat)\n",
    "                    print(\"Список список ребер получен\")\n",
    "                    print(f\"Размер матрицы смежности: {mat.shape}\")\n",
    "                    print(f\"Размер матрицы узлов: {node_feats.shape}\")\n",
    "                    if mat.shape[0] == node_feats.shape[0]:\n",
    "                        count3 += 1\n",
    "                    else:\n",
    "                        count4 += 1\n",
    "\n",
    "                    labels = self._get_node_labels(seq_dict, file)\n",
    "                    # print(labels.nonzero())\n",
    "                    print('Метки получены')\n",
    "                    \n",
    "                    data = Data(x=node_feats, edge_index=edge_index, y = labels)\n",
    "                    data_list.append(data)\n",
    "                    count1 += 1\n",
    "                    \n",
    "                    torch.save(data, self.custom_processed_dir + \"/\" + \n",
    "                             os.path.splitext(os.path.basename(file))[0] + '.pt')\n",
    "                    print(f\"Данные сохранены успешно {file}\")\n",
    "                    print(\"---------------------\")\n",
    "                    # print(count1)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    files_with_errors.append(file)\n",
    "                    print(f\"Ошибка при обработке {file}: {str(e)}\")\n",
    "                    count2 += 1\n",
    "                    # print(count2)\n",
    "                    continue\n",
    "                \n",
    "        self.data_prot = data_list\n",
    "        print(self.custom_processed_dir)\n",
    "        print(f\"\\nИтоговая статистика:\")\n",
    "        print(f\"Успешно обработано файлов: {count1}\")\n",
    "        print(f\"Ошибок при обработке: {count2}\")\n",
    "        print(f\"Количество файлов с которых матрицы смежности и узлов равны {count3}\")\n",
    "        print(f\"Количество файлов с которых матрицы смежности и узлов не равны {count4}\")\n",
    "        print(f\"Размер data_prot: {len(self.data_prot)}\")\n",
    "        print(files_with_errors)\n",
    "    def __len__(self):\n",
    "        return len(self.data_prot)\n",
    "    \n",
    "    # file stands for file path\n",
    "    def __getitem__(self, idx):\n",
    "        # print(idx)\n",
    "        # print(len(self.data_prot))\n",
    "     \n",
    "        return self.data_prot[idx] \n",
    "     \n",
    "    def _get_adjacency(self, ca_coords):\n",
    "        ca_distance = np.linalg.norm(ca_coords[:, None, :] - ca_coords[None, :, :], axis=-1) + np.eye(ca_coords.shape[0])*6\n",
    "        adjacency_matrix = ca_distance < 6\n",
    "        # network = nx.from_numpy_array(mask)\n",
    "        \n",
    "        #прошлый кусок кода, который пока не убираю\n",
    "        # network = molecule.network()  \n",
    "        # mat = nx.adjacency_matrix(network)\n",
    "        # m = mat.todense()\n",
    "        return adjacency_matrix\n",
    "   \n",
    "    def _get_edgeindex(self, adjacency_matrix):\n",
    "        nx_graph = nx.from_numpy_array(adjacency_matrix)\n",
    "        edge_index = np.array(nx_graph.edges()).T\n",
    "        return torch.tensor(edge_index, dtype=torch.long)\n",
    "        \n",
    "        #Прошлый кусок кода\n",
    "        \n",
    "        # a, b = np.nonzero(adjacency_mat > 0)\n",
    "        # edge_index = np.stack((a, b), axis=0)  # Создаем массив (2, num_edges)\n",
    "        # return torch.tensor(edge_index, dtype=torch.long)\n",
    "        # edge_ind = []\n",
    "        \n",
    "    def _get_structure(self, file):\n",
    "        parser = PDB.PDBParser()\n",
    "        structure = parser.get_structure(id, file)\n",
    "        return structure\n",
    "\n",
    "    def _filter_unknown_residues(self, structure):\n",
    "        \"\"\"Создает копию структуры без UNK, нуклеотидов и гетероатомов\"\"\"\n",
    "        filtered_structure = PDB.Structure.Structure(\"filtered\")\n",
    "        \n",
    "        for model in structure:\n",
    "            new_model = PDB.Model.Model(model.id)\n",
    "            for chain in model:\n",
    "                new_chain = PDB.Chain.Chain(chain.id)\n",
    "                for residue in chain:\n",
    "                    resname = residue.get_resname().strip()\n",
    "                    # Пропуск нуклеотидов, UNK и гетероатомов\n",
    "                    if resname in ['A', 'C', 'G', 'T', 'U', 'N'] or resname == 'UNK' or residue.id[0] != ' ':\n",
    "                        continue\n",
    "                    new_chain.add(residue)\n",
    "                new_model.add(new_chain)\n",
    "            filtered_structure.add(new_model)\n",
    "        \n",
    "        return filtered_structure\n",
    "\n",
    "\n",
    "    # Function to get sequence from pdb structure \n",
    "    # Uses structure made using biopython\n",
    "    # Those residues for which symbols are U / X are converted into A\n",
    "    \n",
    "    def _get_sequence_and_ca_coordinates(self, structure):\n",
    "        sequence =\"\"\n",
    "        sequence_dict = {}\n",
    "        count = 0\n",
    "        ca_coords = []\n",
    "        try:\n",
    "            result, _ = freesasa.calcBioPDB(structure)\n",
    "            residue_areas = result.residueAreas()\n",
    "        except:\n",
    "            filtered_structure = self._filter_unknown_residues(structure)\n",
    "            result, _ = freesasa.calcBioPDB(filtered_structure)\n",
    "            residue_areas = result.residueAreas()\n",
    "        sasa_dict = {}\n",
    "    # for model in structure:\n",
    "        for chain in structure[0]:\n",
    "            for residue in chain:\n",
    "                if residue.get_resname() in ressymbl.keys():\n",
    "                    if residue.id[0].startswith('H_'):\n",
    "                        continue\n",
    "                    sequence = sequence + ressymbl[residue.get_resname()]\n",
    "                    if residue.get_id()[2] != ' ':\n",
    "                        sequence_dict[(str(residue.get_id()[1]) + residue.get_id()[2],chain.id)] = (ressymbl[residue.get_resname()],count)\n",
    "                        sasa_dict[(str(residue.get_id()[1]) + residue.get_id()[2],chain.id)] = residue_areas[chain.id][str(residue.get_id()[1]) + residue.get_id()[2]].total\n",
    "                    else:\n",
    "                        sequence_dict[(str(residue.get_id()[1]),chain.id)] = (ressymbl[residue.get_resname()],count)\n",
    "                        sasa_dict[(str(residue.get_id()[1]),chain.id)] = residue_areas[chain.id][str(residue.get_id()[1])].total\n",
    "                    count += 1\n",
    "                    try:                               #Если нет альфа-атома, то добавим координаты бета-атома остова аминокислоты\n",
    "                        ca_atom = residue[\"CA\"]   \n",
    "                        ca_coords.append(ca_atom.coord)\n",
    "                    except:\n",
    "                        try:\n",
    "                            c_atom = residue['C']\n",
    "                            c_coords.append(cb_atom.coord)\n",
    "                        except:\n",
    "                            n_atom = residue['N']\n",
    "                            ca_coords.append(n_atom.coord)\n",
    "                  \n",
    "        return sequence, sequence_dict, np.array(ca_coords), sasa_dict\n",
    "    \n",
    "\n",
    "    # One hot encoding for symbols\n",
    "    def _get_one_hot_symbftrs(self, sequence):\n",
    "        one_hot_symb = np.zeros((len(sequence),len(pro_res_table)))\n",
    "        row = 0\n",
    "        for res in sequence:\n",
    "          col = pro_res_table.index(res)\n",
    "          one_hot_symb[row][col]=1\n",
    "          row +=1\n",
    "        return one_hot_symb\n",
    "    \n",
    "    def _get_phis_chem_properties(self, sequence):\n",
    "        aa_prop_is_seq = np.zeros((len(sequence), len(aa_properties['A'])))\n",
    "        for i, residue in enumerate(sequence):\n",
    "            if residue in aa_properties.keys():\n",
    "                aa_prop_is_seq[i,:] = aa_properties[residue]\n",
    "        return aa_prop_is_seq\n",
    "\n",
    "    def _get_sasa(self, sasa_dict, sequence):\n",
    "        if len(sasa_dict) == len(sequence):\n",
    "            sasa = np.zeros(len(sasa_dict))\n",
    "            for i, key in enumerate(sasa_dict.keys()):\n",
    "                sasa[i] = sasa_dict[key]\n",
    "            return sasa\n",
    "\n",
    "    def _handle_sequence_mismatch(self, sequence, ang_seq, angles):\n",
    "        \"\"\"Синхронизирует углы с основной последовательностью\"\"\"\n",
    "        full_angles = np.zeros((len(sequence), angles.shape[1]))\n",
    "        res_idx = 0\n",
    "        \n",
    "        for i, residue in enumerate(sequence):\n",
    "            if res_idx < len(angles) and residue == ang_seq[res_idx]:\n",
    "                full_angles[i,:] = angles[res_idx]\n",
    "                res_idx += 1\n",
    "            else:\n",
    "                full_angles[i,:] = np.array([1,0,1,0])  # Заполнитель\n",
    "        \n",
    "        return full_angles\n",
    "\n",
    "    def _get_node_ftrs(self, sequence, structure, file, sasa_dict):\n",
    "        try:\n",
    "            one_hot_symb = self._get_one_hot_symbftrs(sequence)\n",
    "            # print('one-hot done')\n",
    "            phis_chem_properties = self._get_phis_chem_properties(sequence)\n",
    "            # print('phis done')\n",
    "            sasa = self._get_sasa(sasa_dict, sequence)\n",
    "            # print('sasa done')\n",
    "            angles, ang_seq = self._get_angles(structure[0])\n",
    "            \n",
    "            # Синхронизация длин последовательностей\n",
    "            if len(sequence) != len(ang_seq):\n",
    "                print('angles rewrite')\n",
    "                angles = self._handle_sequence_mismatch(sequence, ang_seq, angles)\n",
    "            return torch.tensor(np.hstack((one_hot_symb, angles, phis_chem_properties, sasa.reshape(-1,1))), dtype = torch.float)\n",
    "                    # else:\n",
    "                    #     one_hot_symb = self._get_one_hot_symbftrs(ang_seq)\n",
    "                    #     phis_chem_properties = self._get_phis_chem_properties(sequence)\n",
    "                    #     return torch.tensor(np.hstack((one_hot_symb, angles, phis_chem_properties)), dtype = torch.float)\n",
    "        except Exception as e:\n",
    "    \n",
    "                    print(f\"Ошибка при обработке {file}: {str(e)}\") \n",
    "\n",
    "    def _get_angles(self, structure):\n",
    "        angles_trig = []\n",
    "        seq = \"\"\n",
    "\n",
    "        # Проходим по моделям и цепям\n",
    "    # for model in structure:\n",
    "    #     for chain in model:\n",
    "        for chain in structure:    \n",
    "            polypeptides = PDB.CaPPBuilder().build_peptides(chain)\n",
    "            for poly in polypeptides:\n",
    "                seq += poly.get_sequence()\n",
    "                phi_psi = poly.get_phi_psi_list()\n",
    "                phi_psi[0] = (0, phi_psi[0][1])\n",
    "                phi_psi[-1] = (phi_psi[-1][0], 0)\n",
    "                phi_psi = np.array(phi_psi)\n",
    "                try:\n",
    "                    sin_phi = np.sin(phi_psi[:,0])\n",
    "                except TypeError:\n",
    "                    phi_psi = np.array([[0 if value is None else value for value in item] for item in phi_psi])\n",
    "                    sin_phi = np.sin(phi_psi[:,0])\n",
    "                cos_phi = np.cos(phi_psi[:,0])\n",
    "                sin_psi = np.sin(phi_psi[:,1])\n",
    "                cos_psi = np.cos(phi_psi[:,1])\n",
    "                sin_cos_poly = np.column_stack((sin_phi,cos_phi,sin_psi, cos_psi))\n",
    "                angles_trig.append(sin_cos_poly)\n",
    "        return np.vstack(angles_trig), seq\n",
    "\n",
    "    def _get_node_labels(self, seq_dict, file):\n",
    "        y = torch.zeros((len(seq_dict), len(sites_dict.keys())), dtype=torch.long)\n",
    "        for index, row in test_pdb_df[test_pdb_df['PDBID']== os.path.splitext(os.path.basename(file))[0]][['SITE_TYPE','AminoAcidsWithChains','POS_CHAINS']].iterrows():\n",
    "            pos_chain = eval(row['POS_CHAINS'])[0]\n",
    "            if pos_chain == \"YES\":\n",
    "                for amino_acid, position, chain in eval(row['AminoAcidsWithChains']):\n",
    "                    key = (position, chain)\n",
    "                    # print(key)\n",
    "                    if key in seq_dict and seq_dict[key][0] == amino_acid:\n",
    "                        # print(key)\n",
    "                        s_type = sites_dict[row['SITE_TYPE']]\n",
    "                        # print(s_type)\n",
    "                        y[seq_dict[key][1], s_type] = 1\n",
    "            else:\n",
    "                for amino_acid, position in eval(row['AminoAcidsWithChains']):\n",
    "                    new_seq_dict = {key[0]:val for  key,val in zip(seq_dict.keys(),seq_dict.values())}\n",
    "                    key = str(position)\n",
    "                    if key in new_seq_dict.keys() and new_seq_dict[key][0] == amino_acid:\n",
    "                        print(key, new_seq_dict[key], amino_acid)\n",
    "                        s_type = sites_dict[row['SITE_TYPE']]\n",
    "                        y[new_seq_dict[key][1], s_type] = 1\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68d771-0664-4697-8f73-4ba6902af2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset = Test_Graph_Dataset(root=r\"D:/Proteins/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec35102d-1b19-4754-b396-bf21c40e2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0340091b-87aa-43b9-b7ba-79a9f9681965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_index(y):\n",
    "    for i, val in enumerate(y):\n",
    "        # print(i, val)\n",
    "        if val == 1:\n",
    "            return i\n",
    "    return len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1ea73c3-1481-4b41-acd0-134a9e15d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataset:\n",
    "    data.y = torch.tensor([get_target_index(node) for node in data.y], dtype=torch.long)\n",
    "for data in val_dataset:\n",
    "    data.y = torch.tensor([get_target_index(node) for node in data.y], dtype=torch.long)\n",
    "for data in test_dataset:\n",
    "    data.y = torch.tensor([get_target_index(node) for node in data.y], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee3189-7dc3-40cc-a9fe-9c31b2c90dda",
   "metadata": {},
   "source": [
    "# Обучение графовой нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "752a74cc-028f-4f21-9026-eb4be9e47b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=32)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a07a01b-850a-4142-bc24-cd292dcbe161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def calculate_metrics(true_labels, predicted_labels):\n",
    "    # Преобразуйте метки в плоские массивы\n",
    "    true_labels_flat = torch.cat(true_labels).numpy()\n",
    "    predicted_labels_flat = torch.cat(predicted_labels).numpy()\n",
    "    \n",
    "    # Расчет метрик\n",
    "    accuracy = accuracy_score(true_labels_flat, predicted_labels_flat)\n",
    "    \n",
    "    # Расчет метрик для каждого класса\n",
    "    report = classification_report(true_labels_flat, predicted_labels_flat, output_dict=True)\n",
    "    \n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "683fee0d-82fe-45c8-b721-7ec39bd99714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(train_dict, val_dict):\n",
    "    epochs_train = list(train_dict.keys())\n",
    "    losses_train = list(train_dict.values())\n",
    "    losses_val = list(val_dict.values())\n",
    "    \n",
    "    # Создание графика\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(epochs_train, losses_train, marker='o')\n",
    "    plt.plot(epochs_train, losses_val, marker='^')\n",
    "    plt.title('Потери по эпохам')\n",
    "    plt.xlabel('Эпоха')\n",
    "    plt.ylabel('Потеря')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61889408-3b63-468b-8f39-ca693b1d659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import BatchNorm, SAGEConv\n",
    "import torch\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(BatchNorm(hidden_channels))\n",
    "        self.linear1 = nn.Linear(hidden_channels, out_channels)\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, get_embedds=False):\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.leaky_relu(x)\n",
    "            x = F.dropout(x, p=0.2, training=self.training)\n",
    "        if get_embedds:\n",
    "            return x\n",
    "        else:\n",
    "            return self.linear1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1555539-fee1-49bd-8554-17a1486b7660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "classes = torch.cat([data.y for data in train_loader]).numpy()\n",
    "classes_unique = np.unique(classes)\n",
    "class_weights = compute_class_weight(class_weight = 'balanced', y = classes, classes = classes_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f4afbc1-3cd1-4cf7-acda-725d824e67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_layers = 3\n",
    "model = Net(in_channels=train_dataset.num_features, hidden_channels=20,\n",
    "            out_channels=train_dataset.num_classes, num_layers=num_layers).to(device)\n",
    "loss_op = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5461681b-569e-472b-919d-6b16b12efa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ca0a6-dc5c-4419-b9a4-06ce58be790e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index).float(), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for i, data in enumerate(loader):\n",
    "        data.to(device)\n",
    "        outputs = model(data.x, data.edge_index)        \n",
    "        loss = loss_op(outputs.float(), data.y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        # _, predicted = torch.max(outputs, dim=1)\n",
    "        # predicted_labels.append(predicted)\n",
    "    \n",
    "    val_loss_avg = val_loss / len(loader)\n",
    "    return val_loss_avg\n",
    "    # Рассчитайте метрики\n",
    "    # accuracy, report = calculate_metrics(val_true, predicted_labels)\n",
    "\n",
    "def predict_labels(data):\n",
    "        model.eval()\n",
    "        predicted_labels_list = []\n",
    "        with torch.no_grad():\n",
    "            for graph in data:\n",
    "                outputs = model(graph.to(device).x, graph.to(device).edge_index)  # Передаем каждый граф отдельно\n",
    "                \n",
    "                predicted_labels = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                predicted_labels_list.append(predicted_labels)\n",
    "        \n",
    "        return predicted_labels_list\n",
    "        \n",
    "def get_embedds(data):\n",
    "    model.eval()\n",
    "    predicted_labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for graph in data:\n",
    "            outputs = model(graph.to(device).x, graph.to(device).edge_index, get_embedds=True)  # Передаем каждый граф отдельно\n",
    "            predicted_labels_list.append(outputs)\n",
    "    \n",
    "    return predicted_labels_list\n",
    "\n",
    "\n",
    "times = []\n",
    "train_losses = {}\n",
    "val_losses = {}\n",
    "for epoch in range(1, 51):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,  # количество выполненных эпох\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,  # значение функции потерь\n",
    "    }\n",
    "    os.makedirs(f'D:/Proteins/model_num_layers{num_layers}', exist_ok=True)\n",
    "    torch.save(checkpoint, f'D:/Proteins/model_num_layers{num_layers}/pitstop{epoch}.pth')\n",
    "    train_losses[epoch] = loss\n",
    "    val_l = validate(val_loader)\n",
    "    val_losses[epoch] = val_l\n",
    "    # test_f1 = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_l:.4f}')\n",
    "    if epoch % 10 == 0:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
    "plot_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8afce405-58d9-4d71-ad53-38e31fe57d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = get_embedds(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a5cbea3-fd33-416d-8816-37cf70bd5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = torch.cat(train_embeds,0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8cc5be4-78d7-424b-b9d1-62cbc56fa337",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds_df = pd.DataFrame(train_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "242892f1-7b79-4ecb-96a8-f51cb75a74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds_df['target'] = torch.cat(train_true).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0ea8cd6-5d65-473a-ad23-2775a3649ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.126747</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.135243</td>\n",
       "      <td>0.228443</td>\n",
       "      <td>0.035914</td>\n",
       "      <td>0.051140</td>\n",
       "      <td>0.113129</td>\n",
       "      <td>0.541479</td>\n",
       "      <td>0.163079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>0.018419</td>\n",
       "      <td>0.070321</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.152599</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.126592</td>\n",
       "      <td>0.080979</td>\n",
       "      <td>-0.000542</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003797</td>\n",
       "      <td>-0.001753</td>\n",
       "      <td>0.048125</td>\n",
       "      <td>-0.001851</td>\n",
       "      <td>-0.000344</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>0.036572</td>\n",
       "      <td>-0.003471</td>\n",
       "      <td>-0.006198</td>\n",
       "      <td>0.333027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000665</td>\n",
       "      <td>-0.007981</td>\n",
       "      <td>0.148040</td>\n",
       "      <td>-0.001369</td>\n",
       "      <td>0.246156</td>\n",
       "      <td>-0.003534</td>\n",
       "      <td>0.161483</td>\n",
       "      <td>0.164086</td>\n",
       "      <td>0.213851</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.383381</td>\n",
       "      <td>0.180651</td>\n",
       "      <td>-0.000150</td>\n",
       "      <td>0.042861</td>\n",
       "      <td>0.036018</td>\n",
       "      <td>0.086786</td>\n",
       "      <td>-0.001451</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>0.394632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005355</td>\n",
       "      <td>-0.011930</td>\n",
       "      <td>0.192516</td>\n",
       "      <td>0.278623</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>0.281599</td>\n",
       "      <td>0.145916</td>\n",
       "      <td>0.145331</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.322193</td>\n",
       "      <td>-0.005495</td>\n",
       "      <td>0.033965</td>\n",
       "      <td>-0.003240</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.004374</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>0.296459</td>\n",
       "      <td>-0.006078</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200460</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>-0.004697</td>\n",
       "      <td>0.237077</td>\n",
       "      <td>0.223938</td>\n",
       "      <td>0.112776</td>\n",
       "      <td>0.131771</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.277039</td>\n",
       "      <td>0.256184</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>0.406127</td>\n",
       "      <td>0.423550</td>\n",
       "      <td>-0.002068</td>\n",
       "      <td>0.157232</td>\n",
       "      <td>0.287830</td>\n",
       "      <td>-0.005377</td>\n",
       "      <td>-0.004623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325236</td>\n",
       "      <td>0.041335</td>\n",
       "      <td>-0.006421</td>\n",
       "      <td>-0.004293</td>\n",
       "      <td>0.029904</td>\n",
       "      <td>0.173493</td>\n",
       "      <td>-0.001518</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.000460</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.126747  0.088400 -0.000347  0.135243  0.228443  0.035914  0.051140   \n",
       "1 -0.003797 -0.001753  0.048125 -0.001851 -0.000344 -0.000793  0.036572   \n",
       "2  0.383381  0.180651 -0.000150  0.042861  0.036018  0.086786 -0.001451   \n",
       "3  0.322193 -0.005495  0.033965 -0.003240 -0.000068 -0.004374 -0.001248   \n",
       "4  0.277039  0.256184 -0.001456  0.406127  0.423550 -0.002068  0.157232   \n",
       "\n",
       "          7         8         9  ...        11        12        13        14  \\\n",
       "0  0.113129  0.541479  0.163079  ... -0.001117  0.018419  0.070321  0.038027   \n",
       "1 -0.003471 -0.006198  0.333027  ... -0.000665 -0.007981  0.148040 -0.001369   \n",
       "2  0.325700 -0.000034  0.394632  ... -0.005355 -0.011930  0.192516  0.278623   \n",
       "3  0.296459 -0.006078  0.001239  ...  0.200460  0.013263 -0.000394 -0.004697   \n",
       "4  0.287830 -0.005377 -0.004623  ...  0.325236  0.041335 -0.006421 -0.004293   \n",
       "\n",
       "         15        16        17        18        19  target  \n",
       "0  0.152599 -0.000022  0.126592  0.080979 -0.000542      11  \n",
       "1  0.246156 -0.003534  0.161483  0.164086  0.213851      11  \n",
       "2 -0.000201  0.281599  0.145916  0.145331 -0.005085      11  \n",
       "3  0.237077  0.223938  0.112776  0.131771 -0.000244      11  \n",
       "4  0.029904  0.173493 -0.001518 -0.001641 -0.000460      11  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "afc1935c-ee8f-40a1-a41d-aaf868366cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.403704</td>\n",
       "      <td>-0.001924</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.126324</td>\n",
       "      <td>0.413402</td>\n",
       "      <td>-0.002392</td>\n",
       "      <td>0.146437</td>\n",
       "      <td>0.392951</td>\n",
       "      <td>-0.002192</td>\n",
       "      <td>0.231975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107337</td>\n",
       "      <td>-0.000375</td>\n",
       "      <td>-0.003272</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.261667</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.002223</td>\n",
       "      <td>-0.000816</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.419015</td>\n",
       "      <td>-0.002151</td>\n",
       "      <td>0.093981</td>\n",
       "      <td>-0.005200</td>\n",
       "      <td>0.453908</td>\n",
       "      <td>-0.000530</td>\n",
       "      <td>0.074189</td>\n",
       "      <td>0.394629</td>\n",
       "      <td>-0.019187</td>\n",
       "      <td>0.570207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308640</td>\n",
       "      <td>-0.008045</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>-0.001636</td>\n",
       "      <td>0.053601</td>\n",
       "      <td>0.392125</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>-0.002089</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.393625</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.029989</td>\n",
       "      <td>-0.002750</td>\n",
       "      <td>0.327608</td>\n",
       "      <td>0.101878</td>\n",
       "      <td>0.112841</td>\n",
       "      <td>0.337220</td>\n",
       "      <td>-0.005666</td>\n",
       "      <td>0.474712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024433</td>\n",
       "      <td>-0.006729</td>\n",
       "      <td>0.244288</td>\n",
       "      <td>0.202892</td>\n",
       "      <td>0.099007</td>\n",
       "      <td>0.356002</td>\n",
       "      <td>0.150365</td>\n",
       "      <td>0.167605</td>\n",
       "      <td>0.036087</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.011047</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>-0.021760</td>\n",
       "      <td>-0.019147</td>\n",
       "      <td>-0.007775</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>0.075258</td>\n",
       "      <td>-0.009342</td>\n",
       "      <td>-0.009849</td>\n",
       "      <td>0.459967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007403</td>\n",
       "      <td>-0.009316</td>\n",
       "      <td>0.387904</td>\n",
       "      <td>0.121147</td>\n",
       "      <td>0.355630</td>\n",
       "      <td>-0.008478</td>\n",
       "      <td>0.181939</td>\n",
       "      <td>0.348604</td>\n",
       "      <td>0.050303</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.010412</td>\n",
       "      <td>-0.008552</td>\n",
       "      <td>-0.013178</td>\n",
       "      <td>-0.010227</td>\n",
       "      <td>-0.008046</td>\n",
       "      <td>-0.007320</td>\n",
       "      <td>0.100035</td>\n",
       "      <td>-0.009387</td>\n",
       "      <td>-0.003819</td>\n",
       "      <td>0.250870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005823</td>\n",
       "      <td>-0.010120</td>\n",
       "      <td>0.384825</td>\n",
       "      <td>0.244414</td>\n",
       "      <td>0.281254</td>\n",
       "      <td>-0.008412</td>\n",
       "      <td>0.199354</td>\n",
       "      <td>0.257666</td>\n",
       "      <td>0.135895</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.403704 -0.001924  0.004701  0.126324  0.413402 -0.002392  0.146437   \n",
       "1  0.419015 -0.002151  0.093981 -0.005200  0.453908 -0.000530  0.074189   \n",
       "2  0.393625  0.004150  0.029989 -0.002750  0.327608  0.101878  0.112841   \n",
       "3 -0.011047 -0.010926 -0.021760 -0.019147 -0.007775 -0.013697  0.075258   \n",
       "4 -0.010412 -0.008552 -0.013178 -0.010227 -0.008046 -0.007320  0.100035   \n",
       "\n",
       "          7         8         9  ...        11        12        13        14  \\\n",
       "0  0.392951 -0.002192  0.231975  ...  0.107337 -0.000375 -0.003272 -0.002262   \n",
       "1  0.394629 -0.019187  0.570207  ...  0.308640 -0.008045 -0.000411 -0.001636   \n",
       "2  0.337220 -0.005666  0.474712  ...  0.024433 -0.006729  0.244288  0.202892   \n",
       "3 -0.009342 -0.009849  0.459967  ... -0.007403 -0.009316  0.387904  0.121147   \n",
       "4 -0.009387 -0.003819  0.250870  ... -0.005823 -0.010120  0.384825  0.244414   \n",
       "\n",
       "         15        16        17        18        19  target  \n",
       "0  0.009248  0.261667 -0.001019 -0.002223 -0.000816      11  \n",
       "1  0.053601  0.392125  0.029525  0.012760 -0.002089      11  \n",
       "2  0.099007  0.356002  0.150365  0.167605  0.036087      11  \n",
       "3  0.355630 -0.008478  0.181939  0.348604  0.050303      11  \n",
       "4  0.281254 -0.008412  0.199354  0.257666  0.135895      11  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeds = get_embedds(test_loader)\n",
    "test_embeds = torch.cat(test_embeds,0).numpy()\n",
    "test_embeds_df = pd.DataFrame(test_embeds)\n",
    "test_embeds_df['target'] = torch.cat(y_true).numpy()\n",
    "test_embeds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1078b0f1-a057-489b-bb6f-f00464b54869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbea2a4-9199-4b94-bb23-77528d2bc258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,    # Количество деревьев\n",
    "    learning_rate=0.1,   # Скорость обучения\n",
    "    max_depth=3,         # Макс. глубина деревьев\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(train_embeds_df.iloc[:,:-1],train_embeds_df.iloc[:,-1])\n",
    "\n",
    "# 5. Прогнозирование и оценка\n",
    "y_pred = gb_model.predict(test_embeds_df.iloc[:,:-1])\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_embeds_df.iloc[:,-1], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed8204b5-031d-4aa3-b11b-698748c0d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds_df.to_csv('train_embedds_df.csv', index=False)\n",
    "test_embeds_df.to_csv('test_embedds_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf2565-cc55-429f-a270-ffaa518d5e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
